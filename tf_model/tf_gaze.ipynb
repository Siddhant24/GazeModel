{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines, and matplotlib for plotting\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "# filter out the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades_path = '/usr/share/opencv/haarcascades/'\n",
    "face_cascade = cv2.CascadeClassifier(cascades_path + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cascades_path + 'haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def current_time():\n",
    "    return int(round(time.time() * 1000))\n",
    "\n",
    "def get_right_left_eyes(roi_gray):\n",
    "    # sort descending\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    eyes_sorted_by_size = sorted(eyes, key=lambda x: -x[2])\n",
    "    largest_eyes = eyes_sorted_by_size[:2]\n",
    "    # sort by x position\n",
    "    largest_eyes.sort(key=lambda x: x[0])\n",
    "    return largest_eyes\n",
    "\n",
    "def extract_face_features(face, img, gray):\n",
    "    [x,y,w,h] = face\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    face_image = np.copy(img[y:y+h, x:x+w])\n",
    "    \n",
    "    eyes = get_right_left_eyes(roi_gray)\n",
    "    eye_images = []\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        eye_images.append(np.copy(img[y+ey:y+ey+eh,x+ex:x+ex+ew]))\n",
    "                \n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        \n",
    "    \n",
    "    return face_image, eye_images\n",
    "\n",
    "def get_face_grid(face, frameW, frameH, gridSize):\n",
    "    faceX,faceY,faceW,faceH = face\n",
    "\n",
    "    return faceGridFromFaceRect(frameW, frameH, gridSize, gridSize, faceX, faceY, faceW, faceH, False)\n",
    "\n",
    "def extract_image_features(full_img_path):\n",
    "    img = cv2.imread(full_img_path)\n",
    "    start_ms = current_time()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_detections = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    faces = []\n",
    "    face_features = []\n",
    "    for [x,y,w,h] in face_detections:\n",
    "        face = [x, y, w, h]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        face_image, eye_images = extract_face_features(face, img, gray)\n",
    "        face_grid = get_face_grid(face, img.shape[1], img.shape[0], 25)\n",
    "        \n",
    "        faces.append(face)\n",
    "        face_features.append([face_image, eye_images, face_grid])\n",
    "    \n",
    "    duration_ms = current_time() - start_ms\n",
    "    print(\"Face and eye extraction took: \", str(duration_ms / 1000) + \"s\")\n",
    "    \n",
    "    return img, faces, face_features\n",
    "\n",
    "gridSize = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face and eye extraction took:  0.141s\n"
     ]
    }
   ],
   "source": [
    "img, faces, face_features = extract_image_features('../appleFace00002/00000.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is converted from https://github.com/CSAILVision/GazeCapture/blob/master/code/faceGridFromFaceRect.m\n",
    "\n",
    "# Given face detection data, generate face grid data.\n",
    "#\n",
    "# Input Parameters:\n",
    "# - frameW/H: The frame in which the detections exist\n",
    "# - gridW/H: The size of the grid (typically same aspect ratio as the\n",
    "#     frame, but much smaller)\n",
    "# - labelFaceX/Y/W/H: The face detection (x and y are 0-based image\n",
    "#     coordinates)\n",
    "# - parameterized: Whether to actually output the grid or just the\n",
    "#     [x y w h] of the 1s square within the gridW x gridH grid.\n",
    "\n",
    "def faceGridFromFaceRect(frameW, frameH, gridW, gridH, labelFaceX, labelFaceY, labelFaceW, labelFaceH, parameterized):\n",
    "\n",
    "    scaleX = gridW / frameW\n",
    "    scaleY = gridH / frameH\n",
    "    \n",
    "    if parameterized:\n",
    "      labelFaceGrid = np.zeros(4)\n",
    "    else:\n",
    "      labelFaceGrid = np.zeros(gridW * gridH)\n",
    "    \n",
    "    grid = np.zeros((gridH, gridW))\n",
    "\n",
    "    # Use one-based image coordinates.\n",
    "    xLo = round(labelFaceX * scaleX)\n",
    "    yLo = round(labelFaceY * scaleY)\n",
    "    w = round(labelFaceW * scaleX)\n",
    "    h = round(labelFaceH * scaleY)\n",
    "\n",
    "    if parameterized:\n",
    "        labelFaceGrid = [xLo, yLo, w, h]\n",
    "    else:\n",
    "        xHi = xLo + w\n",
    "        yHi = yLo + h\n",
    "\n",
    "        # Clamp the values in the range.\n",
    "        xLo = int(min(gridW, max(0, xLo)))\n",
    "        xHi = int(min(gridW, max(0, xHi)))\n",
    "        yLo = int(min(gridH, max(0, yLo)))\n",
    "        yHi = int(min(gridH, max(0, yHi)))\n",
    "\n",
    "        faceLocation = np.ones((yHi - yLo, xHi - xLo))\n",
    "        grid[yLo:yHi, xLo:xHi] = faceLocation\n",
    "\n",
    "        # Flatten the grid.\n",
    "        grid = np.transpose(grid)\n",
    "        labelFaceGrid = grid.flatten()\n",
    "        \n",
    "    return labelFaceGrid\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_title_and_hide_axis(title):\n",
    "    plt.title(title)\n",
    "    plt.axes().get_xaxis().set_visible(False)\n",
    "    plt.axes().get_yaxis().set_visible(False)\n",
    "\n",
    "def render_face_grid(face_grid):\n",
    "    to_print = np.copy(face_grid)\n",
    "    result_image = np.copy(to_print).reshape(25, 25).transpose()\n",
    "    plt.figure()\n",
    "    set_title_and_hide_axis('Face grid')\n",
    "#     print(result_image.shape)\n",
    "    plt.imshow(result_image)\n",
    "\n",
    "def show_extraction_results(img, faces, face_features):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    set_title_and_hide_axis('Original image and extracted features')\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), interpolation=\"bicubic\")\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        print('Face #' + str(i))\n",
    "        #print('i', face, i)\n",
    "        face_image, eye_images, face_grid = face_features[i]\n",
    "        plt.figure()\n",
    "        set_title_and_hide_axis('Extracted face image')\n",
    "        plt.imshow(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB), interpolation=\"bicubic\")\n",
    "        plt.figure()\n",
    "        #print('face image after extraction')\n",
    "        render_face_grid(face_grid)\n",
    "\n",
    "        for eye_image in eye_images:\n",
    "            plt.figure()\n",
    "\n",
    "            #print('eye image after extraction')\n",
    "            set_title_and_hide_axis('Extracted eye image')\n",
    "            plt.imshow(cv2.cvtColor(eye_image, cv2.COLOR_BGR2RGB), interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face and eye extraction took:  0.018s\n"
     ]
    }
   ],
   "source": [
    "img, faces, face_features = extract_image_features('../appleFace00002/00000.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "from tensorflow.python.platform import gfile\n",
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "#     with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "#         graph_def = tf.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], \"./\")\n",
    "    return sess.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./variables/variables'\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2/tensor_names\n",
      "save/ShardedFilename/shard\n",
      "save/num_shards\n",
      "save/StringJoin/inputs_1\n",
      "save/Const\n",
      "save/RestoreV2\n",
      "save/StringJoin\n",
      "save/ShardedFilename\n",
      "dense_6/bias\n",
      "save/Assign_36\n",
      "dense_6/bias/read\n",
      "dense_6/bias/Initializer/Const\n",
      "dense_6/bias/Assign\n",
      "dense_6/kernel\n",
      "save/Assign_37\n",
      "dense_6/kernel/read\n",
      "dense_6/kernel/Initializer/Const\n",
      "dense_6/kernel/Assign\n",
      "Flatten_7/flatten/Reshape/shape/1\n",
      "Flatten_7/flatten/strided_slice/stack_2\n",
      "Flatten_7/flatten/strided_slice/stack_1\n",
      "Flatten_7/flatten/strided_slice/stack\n",
      "dense_5/bias\n",
      "save/Assign_34\n",
      "dense_5/bias/read\n",
      "dense_5/bias/Initializer/Const\n",
      "dense_5/bias/Assign\n",
      "dense_5/kernel\n",
      "save/Assign_35\n",
      "dense_5/kernel/read\n",
      "dense_5/kernel/Initializer/Const\n",
      "dense_5/kernel/Assign\n",
      "Flatten_6/flatten/Reshape/shape/1\n",
      "Flatten_6/flatten/strided_slice/stack_2\n",
      "Flatten_6/flatten/strided_slice/stack_1\n",
      "Flatten_6/flatten/strided_slice/stack\n",
      "concat2/axis\n",
      "dense_4/bias\n",
      "save/Assign_32\n",
      "dense_4/bias/read\n",
      "dense_4/bias/Initializer/Const\n",
      "dense_4/bias/Assign\n",
      "dense_4/kernel\n",
      "save/Assign_33\n",
      "dense_4/kernel/read\n",
      "dense_4/kernel/Initializer/Const\n",
      "dense_4/kernel/Assign\n",
      "Flatten_5/flatten/Reshape/shape/1\n",
      "Flatten_5/flatten/strided_slice/stack_2\n",
      "Flatten_5/flatten/strided_slice/stack_1\n",
      "Flatten_5/flatten/strided_slice/stack\n",
      "dense_3/bias\n",
      "save/Assign_30\n",
      "dense_3/bias/read\n",
      "dense_3/bias/Initializer/Const\n",
      "dense_3/bias/Assign\n",
      "dense_3/kernel\n",
      "save/Assign_31\n",
      "dense_3/kernel/read\n",
      "dense_3/kernel/Initializer/Const\n",
      "dense_3/kernel/Assign\n",
      "dense_2/bias\n",
      "save/Assign_28\n",
      "dense_2/bias/read\n",
      "dense_2/bias/Initializer/Const\n",
      "dense_2/bias/Assign\n",
      "dense_2/kernel\n",
      "save/Assign_29\n",
      "dense_2/kernel/read\n",
      "dense_2/kernel/Initializer/Const\n",
      "dense_2/kernel/Assign\n",
      "Flatten_4/flatten/Reshape/shape/1\n",
      "Flatten_4/flatten/strided_slice/stack_2\n",
      "Flatten_4/flatten/strided_slice/stack_1\n",
      "Flatten_4/flatten/strided_slice/stack\n",
      "Flatten_3/flatten/Reshape/shape/1\n",
      "Flatten_3/flatten/strided_slice/stack_2\n",
      "Flatten_3/flatten/strided_slice/stack_1\n",
      "Flatten_3/flatten/strided_slice/stack\n",
      "concat1/axis\n",
      "conv4_f_bias\n",
      "save/Assign_18\n",
      "conv4_f_bias/read\n",
      "conv4_f_bias/initial_value\n",
      "conv4_f_bias/Assign\n",
      "convolution_14/dilation_rate\n",
      "conv4_f_weight\n",
      "save/Assign_19\n",
      "conv4_f_weight/read\n",
      "conv4_f_weight/initial_value\n",
      "conv4_f_weight/Assign\n",
      "conv4_r_bias\n",
      "save/Assign_22\n",
      "conv4_r_bias/read\n",
      "conv4_r_bias/initial_value\n",
      "conv4_r_bias/Assign\n",
      "convolution_13/dilation_rate\n",
      "conv4_r_weight\n",
      "save/Assign_23\n",
      "conv4_r_weight/read\n",
      "conv4_r_weight/initial_value\n",
      "conv4_r_weight/Assign\n",
      "conv4_l_bias\n",
      "save/Assign_20\n",
      "conv4_l_bias/read\n",
      "conv4_l_bias/initial_value\n",
      "conv4_l_bias/Assign\n",
      "convolution_12/dilation_rate\n",
      "conv4_l_weight\n",
      "save/Assign_21\n",
      "conv4_l_weight/read\n",
      "conv4_l_weight/initial_value\n",
      "conv4_l_weight/Assign\n",
      "conv3_f_bias\n",
      "save/Assign_13\n",
      "conv3_f_bias/read\n",
      "conv3_f_bias/initial_value\n",
      "conv3_f_bias/Assign\n",
      "convolution_11/dilation_rate\n",
      "conv3_f_weight\n",
      "save/Assign_14\n",
      "conv3_f_weight/read\n",
      "conv3_f_weight/initial_value\n",
      "conv3_f_weight/Assign\n",
      "Pad_8/paddings\n",
      "conv3_r_bias\n",
      "save/Assign_15\n",
      "conv3_r_bias/read\n",
      "conv3_r_bias/initial_value\n",
      "conv3_r_bias/Assign\n",
      "convolution_10/dilation_rate\n",
      "conv3_r_weight\n",
      "save/Assign_16\n",
      "conv3_r_weight/read\n",
      "conv3_r_weight/initial_value\n",
      "conv3_r_weight/Assign\n",
      "Pad_7/paddings\n",
      "conv3_bias\n",
      "save/Assign_12\n",
      "conv3_bias/read\n",
      "conv3_bias/initial_value\n",
      "conv3_bias/Assign\n",
      "convolution_9/dilation_rate\n",
      "conv3_weight\n",
      "save/Assign_17\n",
      "conv3_weight/read\n",
      "conv3_weight/initial_value\n",
      "conv3_weight/Assign\n",
      "Pad_6/paddings\n",
      "PadV2_5/constant_values\n",
      "PadV2_5/paddings\n",
      "PadV2_4/constant_values\n",
      "PadV2_4/paddings\n",
      "PadV2_3/constant_values\n",
      "PadV2_3/paddings\n",
      "dense_1/bias\n",
      "save/Assign_26\n",
      "dense_1/bias/read\n",
      "dense_1/bias/Initializer/Const\n",
      "dense_1/bias/Assign\n",
      "dense_1/kernel\n",
      "save/Assign_27\n",
      "dense_1/kernel/read\n",
      "dense_1/kernel/Initializer/Const\n",
      "dense_1/kernel/Assign\n",
      "Flatten_2/flatten/Reshape/shape/1\n",
      "Flatten_2/flatten/strided_slice/stack_2\n",
      "Flatten_2/flatten/strided_slice/stack_1\n",
      "Flatten_2/flatten/strided_slice/stack\n",
      "conv2_f_bias\n",
      "save/Assign_7\n",
      "conv2_f_bias/read\n",
      "conv2_f_bias/initial_value\n",
      "conv2_f_bias/Assign\n",
      "concat_2/axis\n",
      "convolution_8/dilation_rate\n",
      "convolution_7/dilation_rate\n",
      "split_5/split_dim\n",
      "Const_5\n",
      "split_4/split_dim\n",
      "Const_4\n",
      "conv2_f_weight\n",
      "save/Assign_8\n",
      "conv2_f_weight/read\n",
      "split_4\n",
      "conv2_f_weight/initial_value\n",
      "conv2_f_weight/Assign\n",
      "Pad_5/paddings\n",
      "conv2_r_bias\n",
      "save/Assign_9\n",
      "conv2_r_bias/read\n",
      "conv2_r_bias/initial_value\n",
      "conv2_r_bias/Assign\n",
      "concat_1/axis\n",
      "convolution_6/dilation_rate\n",
      "convolution_5/dilation_rate\n",
      "split_3/split_dim\n",
      "Const_3\n",
      "split_2/split_dim\n",
      "Const_2\n",
      "conv2_r_weight\n",
      "save/Assign_10\n",
      "conv2_r_weight/read\n",
      "split_2\n",
      "conv2_r_weight/initial_value\n",
      "conv2_r_weight/Assign\n",
      "Pad_4/paddings\n",
      "conv2_bias\n",
      "save/Assign_6\n",
      "conv2_bias/read\n",
      "conv2_bias/initial_value\n",
      "conv2_bias/Assign\n",
      "concat/axis\n",
      "convolution_4/dilation_rate\n",
      "convolution_3/dilation_rate\n",
      "split_1/split_dim\n",
      "Const_1\n",
      "split/split_dim\n",
      "Const\n",
      "conv2_weight\n",
      "save/Assign_11\n",
      "conv2_weight/read\n",
      "split\n",
      "conv2_weight/initial_value\n",
      "conv2_weight/Assign\n",
      "Pad_3/paddings\n",
      "dense/bias\n",
      "save/Assign_24\n",
      "dense/bias/read\n",
      "dense/bias/Initializer/Const\n",
      "dense/bias/Assign\n",
      "dense/kernel\n",
      "save/Assign_25\n",
      "dense/kernel/read\n",
      "dense/kernel/Initializer/Const\n",
      "dense/kernel/Assign\n",
      "PadV2_2/constant_values\n",
      "PadV2_2/paddings\n",
      "PadV2_1/constant_values\n",
      "PadV2_1/paddings\n",
      "PadV2/constant_values\n",
      "PadV2/paddings\n",
      "Flatten_1/flatten/Reshape/shape/1\n",
      "Flatten_1/flatten/strided_slice/stack_2\n",
      "Flatten_1/flatten/strided_slice/stack_1\n",
      "Flatten_1/flatten/strided_slice/stack\n",
      "Flatten/flatten/Reshape/shape/1\n",
      "Flatten/flatten/strided_slice/stack_2\n",
      "Flatten/flatten/strided_slice/stack_1\n",
      "Flatten/flatten/strided_slice/stack\n",
      "conv1_f_bias\n",
      "save/Assign_1\n",
      "conv1_f_bias/read\n",
      "conv1_f_bias/initial_value\n",
      "conv1_f_bias/Assign\n",
      "convolution_2/dilation_rate\n",
      "conv1_f_weight\n",
      "save/Assign_2\n",
      "conv1_f_weight/read\n",
      "conv1_f_weight/initial_value\n",
      "conv1_f_weight/Assign\n",
      "Pad_2/paddings\n",
      "conv1_r_bias\n",
      "save/Assign_3\n",
      "conv1_r_bias/read\n",
      "conv1_r_bias/initial_value\n",
      "conv1_r_bias/Assign\n",
      "convolution_1/dilation_rate\n",
      "conv1_r_weight\n",
      "save/Assign_4\n",
      "conv1_r_weight/read\n",
      "conv1_r_weight/initial_value\n",
      "conv1_r_weight/Assign\n",
      "Pad_1/paddings\n",
      "conv1_bias\n",
      "save/Assign\n",
      "conv1_bias/read\n",
      "conv1_bias/initial_value\n",
      "conv1_bias/Assign\n",
      "convolution/dilation_rate\n",
      "conv1_weight\n",
      "save/Assign_5\n",
      "save/restore_shard\n",
      "save/restore_all\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/MergeV2Checkpoints/checkpoint_prefixes\n",
      "save/MergeV2Checkpoints\n",
      "save/Identity\n",
      "conv1_weight/read\n",
      "conv1_weight/initial_value\n",
      "conv1_weight/Assign\n",
      "init\n",
      "Pad/paddings\n",
      "facegrid\n",
      "Flatten/flatten/Shape\n",
      "Flatten/flatten/strided_slice\n",
      "Flatten/flatten/Reshape/shape\n",
      "Flatten/flatten/Reshape\n",
      "Flatten_1/flatten/Shape\n",
      "Flatten_1/flatten/strided_slice\n",
      "Flatten_1/flatten/Reshape/shape\n",
      "Flatten_1/flatten/Reshape\n",
      "dense/MatMul\n",
      "dense/BiasAdd\n",
      "relufgfc1\n",
      "Flatten_2/flatten/Shape\n",
      "Flatten_2/flatten/strided_slice\n",
      "Flatten_2/flatten/Reshape/shape\n",
      "Flatten_2/flatten/Reshape\n",
      "dense_1/MatMul\n",
      "dense_1/BiasAdd\n",
      "relufgfc2\n",
      "image_face\n",
      "Pad_2\n",
      "convolution_2\n",
      "add_2\n",
      "relu1_f\n",
      "PadV2_2\n",
      "pool1_f\n",
      "norm1_f\n",
      "Pad_5\n",
      "split_5\n",
      "convolution_8\n",
      "convolution_7\n",
      "concat_2\n",
      "add_5\n",
      "relu2_f\n",
      "PadV2_5\n",
      "pool2_f\n",
      "norm2_f\n",
      "Pad_8\n",
      "convolution_11\n",
      "add_8\n",
      "relu3_f\n",
      "convolution_14\n",
      "add_11\n",
      "relu4_f\n",
      "Flatten_3/flatten/Shape\n",
      "Flatten_3/flatten/strided_slice\n",
      "Flatten_3/flatten/Reshape/shape\n",
      "Flatten_3/flatten/Reshape\n",
      "dense_2/MatMul\n",
      "dense_2/BiasAdd\n",
      "relufc1_f\n",
      "Flatten_5/flatten/Shape\n",
      "Flatten_5/flatten/strided_slice\n",
      "Flatten_5/flatten/Reshape/shape\n",
      "Flatten_5/flatten/Reshape\n",
      "dense_4/MatMul\n",
      "dense_4/BiasAdd\n",
      "relufc2_f\n",
      "image_right\n",
      "Pad_1\n",
      "convolution_1\n",
      "add_1\n",
      "relu1_r\n",
      "PadV2_1\n",
      "pool1_r\n",
      "norm1_r\n",
      "Pad_4\n",
      "split_3\n",
      "convolution_6\n",
      "convolution_5\n",
      "concat_1\n",
      "add_4\n",
      "relu2_r\n",
      "PadV2_4\n",
      "pool2_r\n",
      "norm2_r\n",
      "Pad_7\n",
      "convolution_10\n",
      "add_7\n",
      "relu3_r\n",
      "convolution_13\n",
      "add_10\n",
      "relu4_r\n",
      "image_left\n",
      "Pad\n",
      "convolution\n",
      "add\n",
      "relu1\n",
      "PadV2\n",
      "pool1\n",
      "norm1\n",
      "Pad_3\n",
      "split_1\n",
      "convolution_4\n",
      "convolution_3\n",
      "concat\n",
      "add_3\n",
      "relu2\n",
      "PadV2_3\n",
      "pool2\n",
      "norm2\n",
      "Pad_6\n",
      "convolution_9\n",
      "add_6\n",
      "relu3\n",
      "convolution_12\n",
      "add_9\n",
      "relu4_l\n",
      "concat1\n",
      "Flatten_4/flatten/Shape\n",
      "Flatten_4/flatten/strided_slice\n",
      "Flatten_4/flatten/Reshape/shape\n",
      "Flatten_4/flatten/Reshape\n",
      "dense_3/MatMul\n",
      "dense_3/BiasAdd\n",
      "relufc1\n",
      "concat2\n",
      "Flatten_6/flatten/Shape\n",
      "Flatten_6/flatten/strided_slice\n",
      "Flatten_6/flatten/Reshape/shape\n",
      "Flatten_6/flatten/Reshape\n",
      "dense_5/MatMul\n",
      "dense_5/BiasAdd\n",
      "relufc2\n",
      "Flatten_7/flatten/Shape\n",
      "Flatten_7/flatten/strided_slice\n",
      "Flatten_7/flatten/Reshape/shape\n",
      "Flatten_7/flatten/Reshape\n",
      "dense_6/MatMul\n",
      "dense_6/BiasAdd\n",
      "(?, 1, 1, 625)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# We use our \"load_graph\" function\n",
    "graph = load_graph('saved_model.pb')\n",
    "\n",
    "# We can verify that we can access the list of operations in the graph\n",
    "for op in graph.get_operations():\n",
    "    print(op.name)\n",
    "    \n",
    "# tensor = graph.get_tensor_by_name('facegrid:0')\n",
    "# print(tensor.get_shape());\n",
    "\n",
    "# We access the input and output nodes \n",
    "# x = graph.get_tensor_by_name('prefix/Placeholder/inputs_placeholder:0')\n",
    "# y = graph.get_tensor_by_name('prefix/Accuracy/predictions:0')\n",
    "        \n",
    "# We launch a Session\n",
    "#with tf.Session(graph=graph) as sess:\n",
    "    # Note: we don't nee to initialize/restore anything\n",
    "    # There is no Variables in this graph, only hardcoded constants \n",
    "    #y_out = sess.run(y, feed_dict={\n",
    "     #   x: [[3, 5, 7, 4, 5, 1, 1, 1, 1, 1]] # < 45\n",
    "    #})\n",
    "    # I taught a neural net to recognise when a sum of numbers is bigger than 45\n",
    "    # it should return False in this case\n",
    "#print(y_out) # [[ False ]] Yay, it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image_string = tf.read_file(path)\n",
    "    image_decoded = tf.image.decode_image(image_string)\n",
    "    return image_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# for reading .mat files\n",
    "import scipy.io\n",
    "# load the mean images\n",
    "mu_face = scipy.io.loadmat('../models/mean_images/mean_face_224.mat')\n",
    "mu_right = scipy.io.loadmat('../models/mean_images/mean_right_224.mat')\n",
    "mu_left = scipy.io.loadmat('../models/mean_images/mean_left_224.mat')\n",
    "\n",
    "# average over pixels to obtain the mean (RGB) pixel values\n",
    "mu_face = mu_face['image_mean'].mean(0).mean(0)\n",
    "mu_right = mu_right['image_mean'].mean(0).mean(0)  \n",
    "mu_left = mu_left['image_mean'].mean(0).mean(0)  \n",
    "print(mu_face.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_transformer(path):\n",
    "    face_image_decoded = load_image(path)\n",
    "    tf.transpose(face_image_decoded, [2, 0 ,1]);\n",
    "    face_image_decoded = tf.subtract(face_image_decoded, mu_face);\n",
    "    print(face_image_decoded.get_shape())\n",
    "    return face_image_decoded\n",
    "\n",
    "def right_transformer(path):\n",
    "    right_image_decoded = load_image(path)\n",
    "    tf.transpose(right_image_decoded, [2, 0 ,1]);\n",
    "    right_image_decoded = tf.subtract(right_image_decoded, mu_right);\n",
    "    print(right_image_decoded.get_shape())\n",
    "    return right_image_decoded\n",
    "\n",
    "def left_transformer(path):\n",
    "    left_image_decoded = load_image(path)\n",
    "    tf.transpose(left_image_decoded, [2, 0 ,1]);\n",
    "    left_image_decoded = tf.subtract(left_image_decoded, mu_left);\n",
    "    print(left_image_decoded.get_shape())\n",
    "    return left_image_decoded\n",
    "\n",
    "def facegrid_transformer(path):\n",
    "    facegrid_decoded = load_image(path)\n",
    "    tf.transpose(facegrid_decoded);\n",
    "    tf.reshape(facegrid_decoded, [625, 1, 1]);\n",
    "    print(facegrid_decoded.get_shape())\n",
    "    return facegrid_decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
